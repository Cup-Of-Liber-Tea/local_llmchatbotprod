import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer
import re
from threading import Thread
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="HyperCLOVA X SEED ì±—ë´‡",
    page_icon="ğŸ’¬",
    layout="centered"
)

# ì‘ë‹µ ì •ì œ í•¨ìˆ˜ ì •ì˜ (ìŠ¤íŠ¸ë¦¬ë° í›„ ì „ì²´ í…ìŠ¤íŠ¸ì— ì ìš©)
def clean_response(text):
    # íŠ¹ìˆ˜ í† í° ì œê±° (streamerì˜ skip_special_tokens=True ë¡œ ëŒ€ì²´ë  ìˆ˜ ìˆìŒ)
    text = re.sub(r'<\|im_start\|>assistant', '', text) # ì‹œì‘ ë¶€ë¶„ assistant íƒœê·¸ ì œê±°
    text = re.sub(r'<\|im_end\|>', '', text)
    text = re.sub(r'<\|endofturn\|>', '', text)
    text = re.sub(r'<\|stop\|>', '', text)
    text = re.sub(r'<\|pad\|>', '', text)
    text = re.sub(r'<\|unk\|>', '', text)
    text = re.sub(r'<\|mask\|>', '', text)
    text = re.sub(r'<\|sep\|>', '', text)
    text = re.sub(r'<\|cls\|>', '', text)
    
    # í˜¹ì‹œ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” assistant í‚¤ì›Œë“œ ì œê±° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    text = re.sub(r'^\s*assistant\s*', '', text, flags=re.IGNORECASE).strip()
    
    # ë¹ˆ ì¤„ ì œê±°
    text = re.sub(r'\n\s*\n', '\n', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()

    return text

# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (ìºì‹± ì‚¬ìš©)
@st.cache_resource
def load_model(model_size="0.5B"):
    # í† í° ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ìš°ì„  ì‚¬ìš©)
    token = os.getenv("HF_TOKEN") # í™˜ê²½ ë³€ìˆ˜ ìš°ì„  ì‚¬ìš©
    
    if model_size == "0.5B":
        model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B"
    elif model_size == "1.5B":
        model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B"
    elif model_size == "3B": # 3B ëª¨ë¸ ì˜µì…˜ ì¶”ê°€
        model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-3B"
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ 0.5B ì‚¬ìš© ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬
        print(f"Warning: Unsupported model size '{model_size}'. Defaulting to 0.5B.")
        model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-0.5B"
    
    with st.spinner(f"{model_size} ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)
        
        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float16 ì‚¬ìš©
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            token=token, 
            trust_remote_code=True,
            torch_dtype=torch.float16  # ë°˜ì •ë°€ë„(16ë¹„íŠ¸)ë¡œ ë¡œë“œ
        )
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPUë¡œ ì´ë™
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        st.session_state.device = device # ë””ë°”ì´ìŠ¤ ì •ë³´ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        
    return tokenizer, model

def generate_response(chat_history, tokenizer, model, device):
    # ì „ì²´ ì±„íŒ… íˆìŠ¤í† ë¦¬ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ (ìµœê·¼ 5ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€)
    history_dicts = [msg for msg in chat_history if msg["role"] != "system"] # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸í•˜ê³  ì‹œì‘
    recent_history_dicts = history_dicts[-10:] # ìµœê·¼ 5í„´ (ì‚¬ìš©ì+AI = 10ê°œ ë©”ì‹œì§€)
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ (í•­ìƒ ë§¨ ì•ì—)
    system_message = next((msg for msg in chat_history if msg["role"] == "system"), None)
    if system_message:
        recent_history_dicts.insert(0, system_message)
    else: # í˜¹ì‹œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë¹ ì¡Œìœ¼ë©´ ê¸°ë³¸ê°’ ì¶”ê°€
        recent_history_dicts.insert(0, {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."})

    # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
    try:
        inputs = tokenizer.apply_chat_template(
            recent_history_dicts,
            return_tensors="pt", 
            tokenize=True, 
            add_generation_prompt=True,
            return_dict=True
        ).to(device) # ë°”ë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    except Exception as e:
        st.error(f"ì±„íŒ… í…œí”Œë¦¿ ì ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None # ì˜¤ë¥˜ ë°œìƒ ì‹œ streamer ëŒ€ì‹  None ë°˜í™˜
    
    # ìŠ¤íŠ¸ë¦¬ë¨¸ ì„¤ì •
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
    generation_kwargs = dict(
        inputs=inputs["input_ids"],
        attention_mask=inputs.get("attention_mask"), # attention_mask ì¶”ê°€
        streamer=streamer,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
    
    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ìƒì„± ì‹¤í–‰
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # ìŠ¤íŠ¸ë¦¬ë¨¸ ê°ì²´ ë°˜í™˜
    return streamer

def create_chat_ui():
    st.title("HyperCLOVA X SEED ì±—ë´‡")
    
    # ì‚¬ì´ë“œë°” - ëª¨ë¸ ì„ íƒ
    st.sidebar.title("ì„¤ì •")
    model_size = st.sidebar.radio(
        "ëª¨ë¸ í¬ê¸° ì„ íƒ",
        ["0.5B", "1.5B", "3B"], # 3B ì˜µì…˜ ì¶”ê°€
        index=0, # ê¸°ë³¸ 0.5B
        key="model_size_selection", # ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•œ í‚¤
        help="0.5B, 1.5B, 3B ëª¨ë¸ ì¤‘ ì„ íƒ (3BëŠ” ê³ ì‚¬ì–‘ í•„ìš”)"
    )
    
    # ëª¨ë¸ ë¡œë“œ ìƒíƒœ ê´€ë¦¬
    if "model_loaded" not in st.session_state:
        st.session_state.model_loaded = False
    if "current_model_size" not in st.session_state:
        st.session_state.current_model_size = None

    # ëª¨ë¸ ë¡œë“œ ë²„íŠ¼ ë˜ëŠ” ëª¨ë¸ í¬ê¸° ë³€ê²½ ì‹œ ëª¨ë¸ ë¡œë“œ
    if st.sidebar.button("ëª¨ë¸ ë¡œë“œ/ë³€ê²½") or st.session_state.current_model_size != model_size:
        try:
            st.session_state.tokenizer, st.session_state.model = load_model(model_size)
            st.session_state.model_loaded = True
            st.session_state.current_model_size = model_size
            # ëª¨ë¸ ë³€ê²½ ì‹œ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            st.session_state.chat_history = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
            st.sidebar.success(f"{model_size} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        except Exception as e:
            st.sidebar.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            st.session_state.model_loaded = False
            st.session_state.current_model_size = None
        st.rerun() # UI ê°±ì‹ 
        
    # CUDA ì •ë³´ í‘œì‹œ
    st.sidebar.subheader("CUDA ì •ë³´")
    cuda_available = torch.cuda.is_available()
    st.sidebar.write(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if cuda_available else 'âŒ'}")
    if cuda_available and "device" in st.session_state:
        try:
            st.sidebar.write(f"CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(st.session_state.device)}")
        except Exception:
            st.sidebar.write("CUDA ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”"):
        if "chat_history" in st.session_state:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ë‚¨ê¸°ê³  ì´ˆê¸°í™”
            st.session_state.chat_history = [msg for msg in st.session_state.chat_history if msg["role"] == "system"]
        st.rerun() # UI ê°±ì‹ 
    
    # ì„¸ì…˜ ìƒíƒœì— ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆ)
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = [{"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
    # ì±„íŒ… ë‚´ì—­ í‘œì‹œ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œì™¸)
    for message in st.session_state.chat_history:
        if message["role"] == "user":
            with st.chat_message("user", avatar="ğŸ§‘"):
                st.write(message["content"])
        elif message["role"] == "assistant":
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.write(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if st.session_state.model_loaded:
        user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•˜ê³  UIì— í‘œì‹œ
            st.session_state.chat_history.append({"role": "user", "content": user_input})
            with st.chat_message("user", avatar="ğŸ§‘"):
                st.write(user_input)
            
            # ì‘ë‹µ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                streamer = generate_response(
                    st.session_state.chat_history,
                    st.session_state.tokenizer,
                    st.session_state.model,
                    st.session_state.device
                )
                if streamer: # streamerê°€ ì •ìƒì ìœ¼ë¡œ ë°˜í™˜ë˜ì—ˆì„ ë•Œë§Œ ì‹¤í–‰
                    with st.spinner("ìƒê° ì¤‘..."): # ìŠ¤í”¼ë„ˆë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì˜ì—­ê³¼ í•¨ê»˜ ë°°ì¹˜
                        # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ í‘œì‹œí•˜ê³  ì™„ë£Œ í›„ ì „ì²´ í…ìŠ¤íŠ¸ ë°›ê¸°
                        full_response = st.write_stream(streamer)
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„, ì „ì²´ ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                    cleaned_response = clean_response(full_response) # ê°„ë‹¨í•œ ì •ì œ ì ìš©
                    st.session_state.chat_history.append({"role": "assistant", "content": cleaned_response})
                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ì—ëŠ” rerunì´ í•„ìš” ì—†ìŒ (write_streamì´ UI ì—…ë°ì´íŠ¸)
                else:
                    st.error("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.") 
            
    else:
        st.info("ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ë¡œë“œ/ë³€ê²½ ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ HF_TOKEN ì„¤ì • í™•ì¸ (ì„ íƒ ì‚¬í•­)
    if not os.getenv("HF_TOKEN"):
        print("Warning: Hugging Face token (HF_TOKEN) environment variable not set. Using default token.")
    create_chat_ui() 